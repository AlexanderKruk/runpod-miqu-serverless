llama-cpp-python[server]==0.2.23
nvidia-cublas-cu12==12.2.5.6